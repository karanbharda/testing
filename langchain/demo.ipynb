{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "511ff1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of India is New Delhi.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 42, 'total_tokens': 51, 'completion_time': 0.00189537, 'prompt_time': 0.006078445, 'queue_time': 0.045780191, 'total_time': 0.007973815}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_510c177af0', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--53d1e984-cb62-4dd0-b7fc-d893a54c8296-0' usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model = \"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "result = llm.invoke(\"what is the captial of India\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChataOpenAI\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv \n",
    "model = ChatOpenAI(model=\"gpt-4\",temperature = 1.2, max_completion_tokens=10)\n",
    "result = model.invoke(\"write a 5 line poem on cricket \")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace , HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"TinyLlama\"\n",
    "    task=\"text-generation\"\n",
    ")\n",
    "\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "result = model.invoke(\"what is the capital of India\")\n",
    "print(result.content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChathuggingFace  , HuggingFacePipeline \n",
    "import os \n",
    "\n",
    "os.environ['HF_HOME'] = ''\n",
    "\n",
    "llm =HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,\n",
    "        max_new_tokens=100\n",
    "    ) \n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "result = model.invoke(\"what is the captial of India \")\n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a73d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.26\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "embedding = OpenAIEmbeddings(model = 'text-embedding-3-large' , dimensions = 32)\n",
    "result = embedding.embed_query(\"Delhi is the captial of India\")\n",
    "\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894fe941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "embedding = OpenAIEmbeddings(model = 'text-embedding-3-large' , dimensions = 32)\n",
    "documents = []\n",
    "\n",
    "result = embedding.embed_query(documents)\n",
    "\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f866c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "documents = [\n",
    "    \"Delhi is the capital of India\",\n",
    "    \"Kolkata is the capital of West Bengal\",\n",
    "    \"Paris is the capital of France\"\n",
    "]\n",
    "\n",
    "vector = embedding.embed_documents(documents)\n",
    "\n",
    "print(str(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8acda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "chat_template = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system' , 'you are a helpful {domain} expert'),\n",
    "        ('human' , 'Explain in simple terms, what is {topic}')\n",
    "    ]\n",
    ")\n",
    "prompt = chat_template.invoke(\n",
    "    {\n",
    "        'domain' : 'finance',\n",
    "        'topic' : 'stock market'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1840286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='you are a helpful finance expert', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain in simple terms, what is stock market', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8aa61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello. How can I assist you today?\n",
      "AI: Unfortunately, I'm a large language model, I don't have real-time access to the current date. However, I can suggest ways to find out the current date.\n",
      "\n",
      "You can check the date on your device, such as your phone, computer, or watch. Alternatively, you can search for the current date online using a search engine like Google.\n",
      "\n",
      "If you want to know the current date for a specific purpose, feel free to let me know, and I can try to help you with that.\n",
      "AI: I have been trained on a vast amount of text data up to my knowledge cutoff date (December 2023). This means I can provide information and answer questions based on data that was available up to that point.\n",
      "\n",
      "You can ask me questions on a wide range of topics, including:\n",
      "\n",
      "* History: I can provide information on historical events, dates, and figures up to 2023.\n",
      "* Science and technology: I have knowledge on scientific discoveries, technological advancements, and trends up to 2023.\n",
      "* Entertainment: I can give you information on movies, TV shows, music, books, and celebrities up to 2023.\n",
      "* Culture: I can provide information on cultural trends, customs, and practices up to 2023.\n",
      "* Geography: I have knowledge on countries, cities, landmarks, and geographical features up to 2023.\n",
      "* And many more!\n",
      "\n",
      "So, feel free to ask me any questions you have, and I'll do my best to provide you with accurate and helpful information based on my training data.\n",
      "AI: Goodbye. It was nice assisting you. If you ever need help again, don't hesitate to reach out. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv() \n",
    "from langchain_core.messages import AIMessage , HumanMessage , SystemMessage \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model = \"llama-3.1-8b-instant\"\n",
    ")\n",
    "\n",
    "chat_history =[\n",
    "    SystemMessage(content = \"You are a helpful AI assistant\")\n",
    "]\n",
    "while True:\n",
    "    user_input = input(\"you:\")\n",
    "    chat_history.append(HumanMessage(content = user_input))\n",
    "    if user_input == 'exit':\n",
    "        break\n",
    "    response = model.invoke(chat_history)\n",
    "    chat_history.append(response)\n",
    "    print(\"AI:\",response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3ffc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_groq import ChatGroq \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model = \"llama-3.1-8b-instant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "453ba463",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = PromptTemplate(\n",
    "    template = 'Greet this person in 5 language. the name of the person is {name}',\n",
    "    input_variable = ['name']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "483c9565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Greet this person in 5 language. the name of the person is John'\n"
     ]
    }
   ],
   "source": [
    "prompt = template2.invoke({'name':'John'})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a9f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736ff17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are greetings in 5 languages for John:\n",
      "\n",
      "1. English: Hello John.\n",
      "2. Spanish: Hola John.\n",
      "3. French: Bonjour John.\n",
      "4. German: Hallo John.\n",
      "5. Italian: Ciao John.\n",
      "\n",
      "Or if you want to be more formal:\n",
      "\n",
      "1. English: Good morning/afternoon John.\n",
      "2. Spanish: Buenos días/tardes John.\n",
      "3. French: Bonjour John (formal) - or Bonsoir John (formal evening greeting)\n",
      "4. German: Guten Tag/Abend John.\n",
      "5. Italian: Buongiorno/sera John.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "675b0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'nitish', 'age': '35'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict \n",
    "\n",
    "class Person(TypedDict):\n",
    "    name : str \n",
    "    age : int \n",
    "\n",
    "new_person : Person = { 'name' : 'nitish' , 'age' : '35'}\n",
    "\n",
    "print(new_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d5d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cons': ['Weight and size make it uncomfortable for one-handed use', 'Bloatware is a drawback', 'High price tag'], 'key_themes': ['powerful processor', 'stunning camera', 'long battery life', 'weight and size', 'bloatware', 'price tag'], 'name': 'Nitish Singh', 'pros': ['Insanely powerful processor (great for gaming and productivity)', 'Stunning 200MP camera with incredible zoom capabilities', 'Long battery life with fast charging', 'S-Pen support is unique and useful'], 'sentiment': 'pos', 'summary': 'The Samsung Galaxy S24 Ultra is an impressive device with a powerful processor, stunning camera, and long battery life. However, its weight, size, and bloatware are drawbacks. The device is a great option for those who want a high-end smartphone for gaming, productivity, and photography, but may not be worth the high price tag.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq \n",
    "from dotenv import load_dotenv \n",
    "from typing import TypedDict , Annotated , Optional , Literal \n",
    "from pydantic import BaseModel , Field \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatGroq(\n",
    "    model = \"llama-3.1-8b-instant\"\n",
    ")\n",
    "# schema\n",
    "json_schema = {\n",
    "  \"title\": \"Review\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"key_themes\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the key themes discussed in the review in a list\"\n",
    "    },\n",
    "    \"summary\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A brief summary of the review\"\n",
    "    },\n",
    "    \"sentiment\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\"pos\", \"neg\"],\n",
    "      \"description\": \"Return sentiment of the review either negative, positive or neutral\"\n",
    "    },\n",
    "    \"pros\": {\n",
    "      \"type\": [\"array\", \"null\"],\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the pros inside a list\"\n",
    "    },\n",
    "    \"cons\": {\n",
    "      \"type\": [\"array\", \"null\"],\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the cons inside a list\"\n",
    "    },\n",
    "    \"name\": {\n",
    "      \"type\": [\"string\", \"null\"],\n",
    "      \"description\": \"Write the name of the reviewer\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"key_themes\", \"summary\", \"sentiment\"]\n",
    "}\n",
    "\n",
    "structured_model = model.with_structured_output(json_schema)\n",
    "\n",
    "result = structured_model.invoke(\"\"\"I recently upgraded to the Samsung Galaxy S24 Ultra, and I must say, it’s an absolute powerhouse! The Snapdragon 8 Gen 3 processor makes everything lightning fast—whether I’m gaming, multitasking, or editing photos. The 5000mAh battery easily lasts a full day even with heavy use, and the 45W fast charging is a lifesaver.\n",
    "\n",
    "The S-Pen integration is a great touch for note-taking and quick sketches, though I don't use it often. What really blew me away is the 200MP camera—the night mode is stunning, capturing crisp, vibrant images even in low light. Zooming up to 100x actually works well for distant objects, but anything beyond 30x loses quality.\n",
    "\n",
    "However, the weight and size make it a bit uncomfortable for one-handed use. Also, Samsung’s One UI still comes with bloatware—why do I need five different Samsung apps for things Google already provides? The $1,300 price tag is also a hard pill to swallow.\n",
    "\n",
    "Pros:\n",
    "Insanely powerful processor (great for gaming and productivity)\n",
    "Stunning 200MP camera with incredible zoom capabilities\n",
    "Long battery life with fast charging\n",
    "S-Pen support is unique and useful\n",
    "                                 \n",
    "Review by Nitish Singh\n",
    "\"\"\")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8fc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
